# Multi-Document Embedding Search Engine with Caching

A lightweight semantic search engine with intelligent SHA-256 hash-based caching, FAISS vector search, and multiprocessing support.

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104%2B-green)](https://fastapi.tiangolo.com/)

## Features

‚ú® **Smart Caching** - SHA-256 hash-based cache (12x faster rebuilds)  
üîç **Vector Search** - FAISS semantic search with cosine similarity  
‚ö° **Multiprocessing** - 2-3x speedup for embedding generation  
üöÄ **REST API** - FastAPI with auto-generated docs at `/docs`  
üíæ **Persistent** - SQLite cache + FAISS index saved to disk  

## How Caching Works

**SHA-256 Hash-Based Validation:**

1. **Process document** ‚Üí Clean text ‚Üí Compute SHA-256 hash
2. **Check cache** ‚Üí If hash matches ‚Üí Return cached embedding (skip computation)
3. **Generate new** ‚Üí If hash differs ‚Üí Create embedding ‚Üí Save to cache

**Storage:** SQLite database with embeddings as BLOB + SHA-256 hash

**Performance:**
- ‚úÖ 12.5x faster rebuilds (2s vs 25s for 200 docs)
- ‚úÖ 2.5x speedup with multiprocessing on fresh builds
- ‚úÖ Automatic invalidation on document edits

## How to Run Embedding Generation

**Via API (Recommended):**
```powershell
python run_api.py
Invoke-RestMethod -Method POST -Uri http://localhost:8000/rebuild
```

**Force rebuild (ignore cache):**
```powershell
Invoke-RestMethod -Method POST -Uri "http://localhost:8000/rebuild?force=true"
```

**Configuration** (`src/config.py`):
```python
BATCH_SIZE = 32                    # Documents per batch
NUM_WORKERS = 4                    # CPU cores (None = auto-detect)
ENABLE_MULTIPROCESSING = True      # Enable parallel processing
MIN_DOCS_FOR_MULTIPROCESSING = 50  # Minimum docs for multiprocessing
```

## How to Start the API

**Quick Start:**
```powershell
.\venv\Scripts\Activate.ps1
python run_api.py
```

Server runs at: **http://localhost:8000**  
Interactive docs: **http://localhost:8000/docs**

**First-Time Setup:**
```powershell
python run_api.py                                         # Start server
Invoke-RestMethod -Method POST -Uri http://localhost:8000/rebuild   # Build index
Invoke-RestMethod -Uri http://localhost:8000/stats                  # Verify
```

**Alternative (Production):**
```powershell
uvicorn src.api:app --host 0.0.0.0 --port 8000 --workers 4
```

## Folder Structure

```
CodeAtRandom/
‚îú‚îÄ‚îÄ src/                           # Core modules
‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # Settings & constants
‚îÇ   ‚îú‚îÄ‚îÄ preprocessor.py            # Text cleaning
‚îÇ   ‚îú‚îÄ‚îÄ cache_manager.py           # SQLite caching with SHA-256
‚îÇ   ‚îú‚îÄ‚îÄ embedder.py                # sentence-transformers wrapper
‚îÇ   ‚îú‚îÄ‚îÄ search_engine.py           # FAISS vector search
‚îÇ   ‚îî‚îÄ‚îÄ api.py                     # FastAPI endpoints
‚îÇ
‚îú‚îÄ‚îÄ tests/                         # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ test_preprocessor.py
‚îÇ   ‚îú‚îÄ‚îÄ test_cache_manager.py
‚îÇ   ‚îî‚îÄ‚îÄ test_api.py
‚îÇ
‚îú‚îÄ‚îÄ data/                          # Documents (git-ignored)
‚îÇ   ‚îú‚îÄ‚îÄ business/, entertainment/, politics/, sport/, tech/
‚îÇ   ‚îî‚îÄ‚îÄ cache/
‚îÇ       ‚îú‚îÄ‚îÄ embeddings_cache.db    # SQLite cache
‚îÇ       ‚îú‚îÄ‚îÄ faiss_index.bin        # FAISS index
‚îÇ       ‚îî‚îÄ‚îÄ faiss_index_metadata.pkl
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt               # Dependencies
‚îú‚îÄ‚îÄ run_api.py                     # API launcher
‚îî‚îÄ‚îÄ README.md
```

**Key Modules:**
- `config.py` - Settings (model, API, multiprocessing)
- `embedder.py` - Generate embeddings with caching
- `cache_manager.py` - SQLite operations with hash validation
- `search_engine.py` - FAISS search with ranking explanations
- `api.py` - REST endpoints (`/search`, `/rebuild`, `/stats`, `/health`)

## Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Setup Steps

1. **Clone or download the repository**:
   ```powershell
   cd C:\Users\jeetg\OneDrive\Desktop\CodeAtRandom
   ```

2. **Create a virtual environment** (recommended):
   ```powershell
   python -m venv venv
   .\venv\Scripts\Activate.ps1
   ```

3. **Install dependencies**:
   ```powershell
   pip install -r requirements.txt
   ```

   This will install:
   - FastAPI & Uvicorn (API server)
   - sentence-transformers (embedding model)
   - FAISS (vector search)
   - scikit-learn (20 Newsgroups dataset)
   - BeautifulSoup4 (HTML cleaning)
   - pytest (testing)

## Quick Start

### 1. Load Dataset

Load 200 documents from the 20 Newsgroups dataset:

```powershell
python -m scripts.load_data --num-docs 200
```

This creates `doc_001.txt` through `doc_200.txt` in `data/docs/`.

**Options**:
```powershell
# Load specific number of documents
python -m scripts.load_data --num-docs 150

# Use test subset
python -m scripts.load_data --subset test

# Load specific categories only
python -m scripts.load_data --categories sci.space comp.graphics
```

### 2. Start API Server

```powershell
python run_api.py
```

The server will start at `http://localhost:8000`

### 3. Build Search Index

**Option A**: Using curl (PowerShell):
```powershell
Invoke-RestMethod -Uri "http://localhost:8000/rebuild" -Method Post
```

**Option B**: Visit the interactive API docs at `http://localhost:8000/docs` and use the `/rebuild` endpoint.

### 4. Search Documents

**Using curl**:
```powershell
$body = @{
    query = "quantum physics"
    top_k = 5
} | ConvertTo-Json

Invoke-RestMethod -Uri "http://localhost:8000/search" -Method Post -Body $body -ContentType "application/json"
```

**Using Python**:
```python
import requests

response = requests.post(
    "http://localhost:8000/search",
    json={"query": "quantum physics", "top_k": 5}
)

results = response.json()
for result in results['results']:
    print(f"Doc: {result['doc_id']}")
    print(f"Score: {result['score']:.3f}")
    print(f"Preview: {result['preview'][:100]}...")
    print(f"Reasoning: {result['explanation']['match_reasoning']}\n")
```

## API Endpoints

### `POST /search`

Search for similar documents.

**Request**:
```json
{
  "query": "artificial intelligence machine learning",
  "top_k": 5
}
```

**Response**:
```json
{
  "query": "artificial intelligence machine learning",
  "top_k": 5,
  "total_results": 5,
  "results": [
    {
      "doc_id": "doc_042",
      "score": 0.856,
      "preview": "machine learning is a subset of artificial intelligence...",
      "explanation": {
        "matched_keywords": ["artificial", "intelligence", "learning", "machine"],
        "overlap_ratio": 1.0,
        "length_normalized_score": 0.856,
        "cosine_similarity": 0.856,
        "match_reasoning": "This document shows strong semantic similarity (0.86). Matched 4 query terms: artificial, intelligence, learning, machine. Query term overlap: 100.0%."
      }
    }
  ]
}
```

### `POST /rebuild`

Rebuild the search index from documents in `data/docs/`.

**Request**:
```
POST /rebuild?force=false
```

**Query Parameters**:
- `force` (optional): If `true`, regenerate all embeddings regardless of cache

**Response**:
```json
{
  "status": "success",
  "message": "Index rebuilt with 200 documents",
  "stats": {
    "status": "ready",
    "total_documents": 200,
    "index_vectors": 200,
    "embedding_dimension": 384
  }
}
```

### `GET /stats`

Get index statistics.

**Response**:
```json
{
  "status": "ready",
  "total_documents": 200,
  "index_vectors": 200,
  "embedding_dimension": 384,
  "cache_stats": {
    "total_documents": 200,
    "database_size_bytes": 614400,
    "database_size_mb": 0.59
  }
}
```

### `GET /health`

Health check endpoint.

### `DELETE /cache`

Clear the embedding cache.

## Configuration

Edit `src/config.py` to customize:

```python
# Embedding model
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# Search parameters
DEFAULT_TOP_K = 5
MAX_TOP_K = 50

# API settings
API_HOST = "0.0.0.0"
API_PORT = 8000

# Performance
BATCH_SIZE = 32
USE_GPU = False  # Set True if CUDA available
```

## Design Choices

| Component | Choice | Why |
|-----------|--------|-----|
| **Embedding Model** | all-MiniLM-L6-v2 | Lightweight (80MB), fast, 384-dim normalized vectors |
| **Cache Storage** | SQLite + BLOB | Zero-config, ACID compliance, efficient binary storage |
| **Vector Search** | FAISS IndexFlatIP | Exact search, inner product = cosine similarity for normalized vectors |
| **Cache Validation** | SHA-256 hashing | Content-based, collision-resistant, automatic invalidation |
| **API Framework** | FastAPI | Async support, auto-docs, Pydantic validation |
| **Parallelization** | Multiprocessing | Bypass GIL, 2-3x speedup, auto-enabled for 50+ docs |
| **Architecture** | Modular separation | Testable, maintainable, single responsibility per module |

**Key Trade-offs:**
- **all-MiniLM-L6-v2** over all-mpnet-base-v2: 5x smaller, 2x faster, 95% quality
- **SQLite** over Redis: No server needed, persistent, simpler deployment
- **IndexFlatIP** over approximate methods: Exact search for <100K docs
- **Multiprocessing** over threading: True parallelism for CPU-bound embedding generation

## Performance Metrics

Tested on a standard laptop (Intel i5, 8GB RAM):

| Operation | Time | Notes |
|-----------|------|-------|
| Load 200 docs | ~5s | One-time setup |
| First embedding generation | ~30s | Model download + compute |
| Index rebuild (cached) | ~2s | All cache hits |
| Index rebuild (fresh) | ~25s | All cache misses |
| Single query | ~50-200ms | Including ranking explanation |
| Concurrent queries (10) | ~150ms avg | Good scalability |

**Cache Hit Ratio Impact**:
- 100% cache hit: 2s rebuild time
- 0% cache hit: 25s rebuild time
- **~12x speedup** with caching!

## Testing

Run unit tests:

```powershell
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_preprocessor.py -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

## Usage Examples

### Example 1: Technology Search

```python
import requests

response = requests.post(
    "http://localhost:8000/search",
    json={"query": "computer graphics 3D rendering", "top_k": 3}
)

for result in response.json()['results']:
    print(f"{result['doc_id']}: {result['score']:.3f}")
    print(f"  {result['explanation']['match_reasoning']}")
```

### Example 2: Scientific Query

```python
response = requests.post(
    "http://localhost:8000/search",
    json={"query": "space exploration NASA missions", "top_k": 5}
)

# Process results...
```

### Example 3: Cache Performance Test

```python
import time
import requests

# First rebuild (cold cache)
start = time.time()
requests.post("http://localhost:8000/rebuild?force=true")
cold_time = time.time() - start

# Second rebuild (warm cache)
start = time.time()
requests.post("http://localhost:8000/rebuild")
warm_time = time.time() - start

print(f"Cold cache: {cold_time:.2f}s")
print(f"Warm cache: {warm_time:.2f}s")
print(f"Speedup: {cold_time/warm_time:.1f}x")
```

## Troubleshooting

### Issue: "No documents found in data/docs"

**Solution**: Run the data loader first:
```powershell
python -m scripts.load_data --num-docs 200
```

### Issue: "Search engine not initialized"

**Solution**: Wait for server startup to complete or check logs for errors.

### Issue: Model download fails

**Solution**: 
1. Check internet connection
2. Manually download model:
   ```python
   from sentence_transformers import SentenceTransformer
   model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
   ```

### Issue: Slow performance

**Solution**:
- Reduce `BATCH_SIZE` in config if running out of memory
- Enable GPU if available (`USE_GPU = True`)
- Consider using a smaller model

## Extending the System

### Add Custom Documents

1. Place `.txt` files in `data/docs/` with naming pattern `doc_XXX.txt`
2. Rebuild index: `POST /rebuild`

### Use Different Embedding Model

Edit `src/config.py`:
```python
EMBEDDING_MODEL = "sentence-transformers/all-mpnet-base-v2"
EMBEDDING_DIM = 768  # Update dimension
```

### Add Re-ranking

Modify `search_engine.py` to add BM25 or cross-encoder re-ranking:
```python
from rank_bm25 import BM25Okapi

# In SearchEngine.search():
# 1. Get initial results from FAISS
# 2. Re-rank with BM25
# 3. Return combined scores
```

## License

MIT License - see LICENSE file for details.

## Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

## Acknowledgments

- [sentence-transformers](https://www.sbert.net/) for embedding models
- [FAISS](https://github.com/facebookresearch/faiss) for vector search
- [FastAPI](https://fastapi.tiangolo.com/) for the web framework
- [scikit-learn](https://scikit-learn.org/) for the 20 Newsgroups dataset

## Contact

For questions or issues, please open a GitHub issue.

---

**Built with ‚ù§Ô∏è using Python, FastAPI, and sentence-transformers**
